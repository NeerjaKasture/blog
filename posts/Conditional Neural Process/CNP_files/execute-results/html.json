{
  "hash": "c69411e26a0b86239b78950e06f8f28c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Learning Conditional Neural Processes\"\ndescription: \"Visualizing the training of a Conditional Neural Process (CNP) model on sine wave regression using PyTorch.\"\nauthor: \"Neerja Kasture\"\ndate: 2025-08-25\ncategories: [Machine Learning, Neural Processes, PyTorch, Visualization]\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-line-numbers: true\nexecute: true\n---\n\n### Introduction\n\nMeta learning models help us 'learn how to learn'. They are models that can learn new tasks quickly from just a handful of examples. In this blog, we explore a simple but powerful framework for this: the Conditional Neural Process (CNP).\n\n### Imports and Setup\n\n::: {#cell-4 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import Normal\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n```\n:::\n\n\n### Creating training data\n\nInstead of training on a fixed dataset, our model learns by seeing functions. In every training step, we generate a new sine wave of the form $ùë¶=ùëésin(ùë•)$ where the amplitude $ùëé$ is randomly chosen from a uniform distribution in (-2,2). Then we randomly sample points from this function to be context points and target points. We have also added random gaussian noise to our data.\n\n::: {#cell-7 .cell execution_count=2}\n``` {.python .cell-code}\ndef sine(x, a):\n  return a * np.sin(x)\n\ndef create_training_data(func, a=None, num_points=None, num_context=None):\n    if not a:\n      a = np.random.uniform(-2,2) # randomly sample a\n\n    if not num_points:  # if not specified choose a random number of context and target points\n        num_points = np.random.randint(50, 100)\n    if not num_context:\n        num_context = np.random.randint(10, 20)\n\n    x_all = np.random.uniform(-np.pi, np.pi, num_points)\n    y_all = func(x_all, a)\n\n    noise = np.random.normal(0, 0.1, size=y_all.shape) # add noise\n    y_all += noise\n\n    context_indices = np.random.choice(num_points, num_context, replace=False)\n    x_context = x_all[context_indices]\n    y_context = y_all[context_indices]\n\n    target_indices = np.setdiff1d(np.arange(num_points), context_indices)\n    x_target = x_all[target_indices]\n    y_target = y_all[target_indices]\n\n    x_context = torch.tensor(x_context, dtype=torch.float32).unsqueeze(-1).to(device)\n    y_context = torch.tensor(y_context, dtype=torch.float32).unsqueeze(-1).to(device)\n    x_target = torch.tensor(x_target, dtype=torch.float32).unsqueeze(-1).to(device)\n    y_target = torch.tensor(y_target, dtype=torch.float32).unsqueeze(-1).to(device)\n\n    return x_context, y_context, x_target, y_target\n```\n:::\n\n\nLets visualize some of these functions:\n\n::: {#cell-9 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":465}}' outputId='85305640-e804-4d23-8878-a2a6f20efe43' execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](CNP_files/figure-html/cell-4-output-1.png){width=674 height=429}\n:::\n:::\n\n\n### The Model\n\nA CNP has two main components:\n\n1. Encoder : A neural network that takes in context pair $(x_i , y_i)$ and transforms them into representation vector $r_i$. To combine information from all context points, we take an average over all points and get a vector $r$ of same dimension as each vector $r_i$.\n2. Decoder: A neural network that uses the representation $r$ to make predictions at target point $x_t$. We pass in concatenated $[r,x_t]$ as the input and the output is the predicted mean and variance of $y_t$. We normalize the value of sigma to make sure it is positive.\n\n::: {#cell-12 .cell execution_count=4}\n``` {.python .cell-code}\nclass Encoder(nn.Module):\n  def __init__(self, output_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(2, 128) # (x_context, y_context) as input\n    self.fc2 = nn.Linear(128, 128)\n    self.fc3 = nn.Linear(128, output_dim)\n    self.relu = nn.ReLU()\n\n  def forward(self, x):\n    x = self.relu(self.fc1(x))\n    x = self.relu(self.fc2(x))\n    x = self.fc3(x)\n    return x\n\nclass Decoder(nn.Module):\n  def __init__(self,r_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(r_dim+1, 128) # (r vector, x_target) concatenated as input\n    self.fc2 = nn.Linear(128, 2)\n    self.relu = nn.ReLU()\n\n  def forward(self, x):\n    x = self.relu(self.fc1(x))\n    x = self.fc2(x)\n    mu,log_sigma = x.chunk(2, dim=-1)\n    return mu, log_sigma\n\nclass ConditionalNeuralProcess(nn.Module):\n  def __init__(self,r_dim):\n    super().__init__()\n    self.encoder = Encoder(r_dim)\n    self.decoder = Decoder(r_dim)\n\n  def forward(self,context_x,context_y,target_x):\n\n    context_point = torch.cat([context_x, context_y], dim=-1)\n    r_i = self.encoder(context_point)\n    r =torch.mean(r_i,dim=0)\n    num_target = target_x.shape[0]\n    r_expanded = r.expand(num_target, -1)\n    decoder_input = torch.cat([r_expanded, target_x], dim=-1)\n\n    mu, log_sigma = self.decoder(decoder_input)\n    sigma = torch.exp(log_sigma) # variance must be positive\n\n    return mu, sigma\n\nr_dim = 128\nmodel = ConditionalNeuralProcess(r_dim).to(device)\n\n```\n:::\n\n\nThe code below will help us visualize the model inputs and predictions.\n\n::: {#cell-14 .cell execution_count=5}\n``` {.python .cell-code}\ndef visualize_cnp_predictions(model, context_x, context_y, target_x, target_y):\n    model.eval()\n\n    with torch.no_grad():\n        # Generate a dense range of x-values to plot the learned function smoothly\n        x_plot = torch.linspace(-np.pi, np.pi, 500, device=device).unsqueeze(-1)\n\n\n        mu_pred, sigma_pred = model(context_x, context_y, x_plot)\n\n        context_x_np = context_x.cpu().numpy()\n        context_y_np = context_y.cpu().numpy()\n        target_x_np = target_x.cpu().numpy()\n        target_y_np = target_y.cpu().numpy()\n        x_plot_np = x_plot.cpu().numpy()\n        mu_pred_np = mu_pred.cpu().numpy()\n        sigma_pred_np = sigma_pred.cpu().numpy()\n\n    plt.figure(figsize=(10, 6))\n\n    plt.scatter(context_x_np, context_y_np, c='red', label='Context Points', marker='o', s=70)\n    plt.scatter(target_x_np, target_y_np, c='blue', label='True Target Points', marker='x', s=70)\n\n    plt.plot(x_plot_np, mu_pred_np, color='green', linewidth=2, label='Predicted Mean')\n    plt.fill_between(\n        x_plot_np.squeeze(),\n        (mu_pred_np - 2 * sigma_pred_np).squeeze(),\n        (mu_pred_np + 2 * sigma_pred_np).squeeze(),\n        color='green', alpha=0.2, label='2œÉ Uncertainty'\n    )\n\n    plt.title('Conditional Neural Process Predictions')\n    plt.xlabel('X', fontsize=12)\n    plt.ylabel('Y', fontsize=12)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n```\n:::\n\n\nLets sample data for an example function with a = 2. What does our model's prediction look like before training? \n\n::: {#cell-16 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":568}}' outputId='6018018b-efd0-42d7-be33-279c05663268' execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](CNP_files/figure-html/cell-7-output-1.png){width=813 height=526}\n:::\n:::\n\n\n### Training\n\nThe model is trained by minimizing the negative log-likelihood (NLL) ‚Äî encouraging the predicted distributions to assign high probability to the true target values.\n\n::: {#cell-19 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\"}}' outputId='6fd2c68f-173d-4ca8-d202-28b306b980ff' execution_count=7}\n``` {.python .cell-code}\nr_dim = 128\nmodel = ConditionalNeuralProcess(r_dim).to(device)\n\ndef NLL(mu_pred, sigma_pred, target_y):\n    dist = Normal(mu_pred, sigma_pred)\n    log_prob = dist.log_prob(target_y)\n    loss = -torch.mean(log_prob)\n\n    return loss\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nnum_epochs = 10000\nlosses=[]\npredictions_example=[]\nlosses_example=[]\nx_plot_example = np.linspace(-np.pi, np.pi, 500).reshape(-1, 1)\ny_plot_example = sine(x_plot_example, 2) + np.random.normal(0, 0.1, size=x_plot_example.shape)\nx_plot_example = torch.linspace(-np.pi, np.pi, 500, device=device).unsqueeze(-1)\n\nfor epoch in range(num_epochs):\n    x_context, y_context, x_target, y_target = create_training_data(sine)\n    optimizer.zero_grad()\n    mu_pred, sigma_pred = model(x_context, y_context, x_target)\n    loss = NLL(mu_pred, sigma_pred, y_target)\n    losses.append(loss.item())\n\n    loss.backward()\n    optimizer.step()\n\n    if (epoch+1) % 1000 == 0:\n      # visualize_cnp_predictions(model,x_context_example,y_context_example,x_target_example,y_target_example)\n      print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n\n    with torch.no_grad():\n        mu, sigma = model(x_context_example, y_context_example, x_plot_example)\n        predictions_example.append((mu.cpu().numpy(), sigma.cpu().numpy()))\n        losses_example.append(NLL(mu, sigma, torch.tensor(y_plot_example, dtype=torch.float32).to(device)).item())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1000/10000, Loss: -0.332416296005249\nEpoch 2000/10000, Loss: 0.5655556321144104\nEpoch 3000/10000, Loss: -0.34898456931114197\nEpoch 4000/10000, Loss: -0.34581270813941956\nEpoch 5000/10000, Loss: -0.3510619103908539\nEpoch 6000/10000, Loss: -0.6369382739067078\nEpoch 7000/10000, Loss: -0.03096843510866165\nEpoch 8000/10000, Loss: -0.6118342280387878\nEpoch 9000/10000, Loss: -0.32080864906311035\nEpoch 10000/10000, Loss: -0.6325305104255676\n```\n:::\n:::\n\n\nThe losses for the overall training process oscillate a lot but we can see a downward trend, as in the plot below.\n\n::: {#cell-21 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":449}}' outputId='1ed62f48-3315-4b63-fc85-eefd13991b71' execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](CNP_files/figure-html/cell-9-output-1.png){width=600 height=429}\n:::\n:::\n\n\nWe can visualize how the training makes our model converge to the example function using a short animation. Observe how the shaded green area indicating variance becomes smaller as our model becomes more accurate and confident.\n\n\n![](cnp_training.gif){fig-align=\"center\"}\n\nSo, after training, the model is predicting a good approximation of our function.\n\n::: {#cell-26 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":568}}' outputId='cdab9357-7aac-48ab-cc50-9986d637db59' execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](CNP_files/figure-html/cell-11-output-1.png){width=813 height=526}\n:::\n:::\n\n\nI would say that CNPs are a rather simple approximation to neural processes. Their aggregator function being simply a mean may lose information and they do not model uncertainty as well, being a deterministic model. However they are simple to train and perform reasonably well on simple functions such as the sine wave.\n\nCredits to [Kaspar Martens](https://kasparmartens.rbind.io/post/np/) and [Deepmind](https://colab.research.google.com/github/deepmind/neural-processes/blob/master/conditional_neural_process.ipynb#scrollTo=P3LJYP1Qh-jO)  for an excellent tutorial on this topic.\n\n",
    "supporting": [
      "CNP_files"
    ],
    "filters": [],
    "includes": {}
  }
}