[
  {
    "objectID": "posts/first_post/first_post.html",
    "href": "posts/first_post/first_post.html",
    "title": "Hello World üöÄ",
    "section": "",
    "text": "Hi all,\nWith the fruitful use of Youtube and AI tools, I have figured out how Quartro works so here‚Äôs my first post!\nI‚Äôll be writing here about my projects, research work, learnings and thoughts (and maybe an odd poem to bring back the WordPress feels)\nStay tuned!\nNeerja."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Learning Functions with Conditional Neural Processes\n\n\n\nMachine Learning\n\nNeural Processes\n\ntutorial\n\n\n\nImplementing a Conditional Neural Process model to learn the sine wave function\n\n\n\nAug 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello World üöÄ\n\n\n\nintro\n\npersonal\n\n\n\nMy first blog post\n\n\n\nAug 25, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Conditional Neural Process/CNP.html",
    "href": "posts/Conditional Neural Process/CNP.html",
    "title": "Learning Functions with Conditional Neural Processes",
    "section": "",
    "text": "Introduction\nMeta learning models help us ‚Äòlearn how to learn‚Äô. They are models that can learn new tasks quickly from just a handful of examples. In this blog, we explore a simple but powerful framework for this: the Conditional Neural Process.\n\n\nImports and Setup\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import Normal\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\nCreating training data\nInstead of training on a fixed dataset, our model learns by seeing functions. In every training step, we generate a new sine wave of the form \\(ùë¶=ùëésin(ùë•)\\) where the amplitude \\(ùëé\\) is randomly chosen from a uniform distribution in (-2,2). Then we randomly sample points from this function to be context points and target points. We have also added random gaussian noise to our data.\n\ndef sine(x, a):\n  return a * np.sin(x)\n\ndef create_training_data(func, a=None, num_points=None, num_context=None):\n    if not a:\n      a = np.random.uniform(-2,2) # randomly sample a\n\n    if not num_points:  # if not specified choose a random number of context and target points\n        num_points = np.random.randint(50, 100)\n    if not num_context:\n        num_context = np.random.randint(10, 20)\n\n    x_all = np.random.uniform(-np.pi, np.pi, num_points)\n    y_all = func(x_all, a)\n\n    noise = np.random.normal(0, 0.1, size=y_all.shape) # add noise\n    y_all += noise\n\n    context_indices = np.random.choice(num_points, num_context, replace=False)\n    x_context = x_all[context_indices]\n    y_context = y_all[context_indices]\n\n    target_indices = np.setdiff1d(np.arange(num_points), context_indices)\n    x_target = x_all[target_indices]\n    y_target = y_all[target_indices]\n\n    x_context = torch.tensor(x_context, dtype=torch.float32).unsqueeze(-1).to(device)\n    y_context = torch.tensor(y_context, dtype=torch.float32).unsqueeze(-1).to(device)\n    x_target = torch.tensor(x_target, dtype=torch.float32).unsqueeze(-1).to(device)\n    y_target = torch.tensor(y_target, dtype=torch.float32).unsqueeze(-1).to(device)\n\n    return x_context, y_context, x_target, y_target\n\nLets visualize some of these functions:\n\n\n\n\n\n\n\n\n\n\n\nThe Model\nA CNP has two main components:\n\nEncoder : A neural network that takes in context pair \\((x_i , y_i)\\) and transforms them into representation vector \\(r_i\\). To combine information from all context points, we take an average over all points and get a vector \\(r\\) of same dimension as each vector \\(r_i\\).\nDecoder: A neural network that uses the representation \\(r\\) to make predictions at target point \\(x_t\\). We pass in concatenated \\([r,x_t]\\) as the input and the output is the predicted mean and variance of \\(y_t\\). We normalize the value of sigma to make sure it is positive.\n\n\nclass Encoder(nn.Module):\n  def __init__(self, output_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(2, 128) # (x_context, y_context) as input\n    self.fc2 = nn.Linear(128, 128)\n    self.fc3 = nn.Linear(128, output_dim)\n    self.relu = nn.ReLU()\n\n  def forward(self, x):\n    x = self.relu(self.fc1(x))\n    x = self.relu(self.fc2(x))\n    x = self.fc3(x)\n    return x\n\nclass Decoder(nn.Module):\n  def __init__(self,r_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(r_dim+1, 128) # (r vector, x_target) concatenated as input\n    self.fc2 = nn.Linear(128, 2)\n    self.relu = nn.ReLU()\n\n  def forward(self, x):\n    x = self.relu(self.fc1(x))\n    x = self.fc2(x)\n    mu,log_sigma = x.chunk(2, dim=-1)\n    return mu, log_sigma\n\nclass ConditionalNeuralProcess(nn.Module):\n  def __init__(self,r_dim):\n    super().__init__()\n    self.encoder = Encoder(r_dim)\n    self.decoder = Decoder(r_dim)\n\n  def forward(self,context_x,context_y,target_x):\n\n    context_point = torch.cat([context_x, context_y], dim=-1)\n    r_i = self.encoder(context_point)\n    r =torch.mean(r_i,dim=0)\n    num_target = target_x.shape[0]\n    r_expanded = r.expand(num_target, -1)\n    decoder_input = torch.cat([r_expanded, target_x], dim=-1)\n\n    mu, log_sigma = self.decoder(decoder_input)\n    sigma = torch.exp(log_sigma) # variance must be positive\n\n    return mu, sigma\n\nr_dim = 128\nmodel = ConditionalNeuralProcess(r_dim).to(device)\n\nThe code below will help us visualize the model inputs and predictions.\n\ndef visualize_cnp_predictions(model, context_x, context_y, target_x, target_y):\n    model.eval()\n\n    with torch.no_grad():\n        # Generate a dense range of x-values to plot the learned function smoothly\n        x_plot = torch.linspace(-np.pi, np.pi, 500, device=device).unsqueeze(-1)\n\n\n        mu_pred, sigma_pred = model(context_x, context_y, x_plot)\n\n        context_x_np = context_x.cpu().numpy()\n        context_y_np = context_y.cpu().numpy()\n        target_x_np = target_x.cpu().numpy()\n        target_y_np = target_y.cpu().numpy()\n        x_plot_np = x_plot.cpu().numpy()\n        mu_pred_np = mu_pred.cpu().numpy()\n        sigma_pred_np = sigma_pred.cpu().numpy()\n\n    plt.figure(figsize=(10, 6))\n\n    plt.scatter(context_x_np, context_y_np, c='red', label='Context Points', marker='o', s=70)\n    plt.scatter(target_x_np, target_y_np, c='blue', label='True Target Points', marker='x', s=70)\n\n    plt.plot(x_plot_np, mu_pred_np, color='green', linewidth=2, label='Predicted Mean')\n    plt.fill_between(\n        x_plot_np.squeeze(),\n        (mu_pred_np - 2 * sigma_pred_np).squeeze(),\n        (mu_pred_np + 2 * sigma_pred_np).squeeze(),\n        color='green', alpha=0.2, label='Variance'\n    )\n\n    plt.title('Conditional Neural Process Predictions')\n    plt.xlabel('X', fontsize=12)\n    plt.ylabel('Y', fontsize=12)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nLets sample data for an example function with a = 1.5 , What does our model‚Äôs prediction look like before training?\n\n\n\n\n\n\n\n\n\n\n\nTraining\nThe model is trained by minimizing the negative log-likelihood (NLL) ‚Äî encouraging the predicted distributions to assign high probability to the true target values.\n\nr_dim = 128\nmodel = ConditionalNeuralProcess(r_dim).to(device)\n\ndef NLL(mu_pred, sigma_pred, target_y):\n    dist = Normal(mu_pred, sigma_pred)\n    log_prob = dist.log_prob(target_y)\n    loss = -torch.mean(log_prob)\n\n    return loss\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nnum_epochs = 10000\nlosses=[]\npredictions_example=[]\nx_plot_example = torch.linspace(-np.pi, np.pi, 500, device=device).unsqueeze(-1)\n\nfor epoch in range(num_epochs):\n    x_context, y_context, x_target, y_target = create_training_data(sine)\n    optimizer.zero_grad()\n    mu_pred, sigma_pred = model(x_context, y_context, x_target)\n    loss = NLL(mu_pred, sigma_pred, y_target)\n    losses.append(loss.item())\n\n    loss.backward()\n    optimizer.step()\n\n    if (epoch+1) % 1000 == 0:\n      #visualize_cnp_predictions(model,x_context_example,y_context_example,x_target_example,y_target_example)\n      print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n\n    with torch.no_grad():\n        mu, sigma = model(x_context_example, y_context_example, x_plot_example)\n        predictions_example.append((mu.cpu().numpy(), sigma.cpu().numpy()))\n\nEpoch 1000/10000, Loss: 0.47015589475631714\nEpoch 2000/10000, Loss: -0.42924317717552185\nEpoch 3000/10000, Loss: -0.12417580932378769\nEpoch 4000/10000, Loss: -0.6538487076759338\nEpoch 5000/10000, Loss: -0.4501391053199768\nEpoch 6000/10000, Loss: -0.36041417717933655\nEpoch 7000/10000, Loss: -0.09112993627786636\nEpoch 8000/10000, Loss: -0.6509308218955994\nEpoch 9000/10000, Loss: -0.7228361368179321\nEpoch 10000/10000, Loss: -0.17287860810756683\n\n\nThe losses for the overall training process oscillate a lot but we can see a downward trend, as in the plot below.\n\n\n\n\n\n\n\n\n\nWe can visualize how the training makes our model converge to the example function using a short animation. Observe how the shaded green area indicating variance becomes smaller as our model becomes more accurate and confident.\n\n\n\n\n\n\n\nConclusion\nSo, after training, the model is predicting a good approximation of our function.\n\n\n\n\n\n\n\n\n\nI would say that CNPs are a rather simple approximation to neural processes. Their aggregator function being simply a mean may lose information and they do not model uncertainty as well, being a deterministic model. However they are simple to train and perform reasonably well on simple functions such as the sine wave.\n\n\nActive Learning Loop\nThe CNP trained above can help us in choosing the next points to acquire in order to make even better predictions about a particular sine wave. Below is the code for two acquisitions strategies - a random strategy and an uncertainty based strategy which chooses point with maximum variance.\n\ndef random_acquisition(x_context, y_context, x_target, y_target):\n    idx = np.random.randint(len(x_target))\n\n    x = x_target[idx:idx+1]\n    y = y_target[idx:idx+1]\n\n    x_context_new = torch.cat([x_context, x], dim=0)\n    y_context_new = torch.cat([y_context, y], dim=0)\n\n    x_target_new = torch.cat([x_target[:idx], x_target[idx+1:]], dim=0)\n    y_target_new = torch.cat([y_target[:idx], y_target[idx+1:]], dim=0)\n\n    return  x_context_new, y_context_new, x_target_new, y_target_new\n\ndef uncertainty_acquisition(sigma, x_context, y_context, x_target, y_target):\n    idx = torch.argmax(sigma).item()\n\n    x = x_target[idx:idx+1]\n    y = y_target[idx:idx+1]\n\n    x_context_new = torch.cat([x_context, x], dim=0)\n    y_context_new = torch.cat([y_context, y], dim=0)\n\n    x_target_new = torch.cat([x_target[:idx], x_target[idx+1:]], dim=0)\n    y_target_new = torch.cat([y_target[:idx], y_target[idx+1:]], dim=0)\n\n    return  x_context_new, y_context_new, x_target_new, y_target_new\n\nLets try this on the example function above with a=1.5 We start with 2 context points and make one target point become context point per epoch.\n\nnum_epochs = 50\na=1.5\nx_context, y_context, x_target, y_target = create_training_data(sine, a,num_points=100, num_context=2)\n\ndef rmse_loss(y_pred,y_target):\n  return torch.sqrt(torch.mean((y_pred - y_target) ** 2)).item()\n\n\nimport copy\n\nmodel_random = copy.deepcopy(model)\nmodel_uncertainty = copy.deepcopy(model)\n\nopt_random = torch.optim.Adam(model_random.parameters(), lr=1e-3)\nopt_uncertainty = torch.optim.Adam(model_uncertainty.parameters(), lr=1e-3)\n\nx_context_r, y_context_r = x_context.clone(), y_context.clone()\nx_target_r, y_target_r = x_target.clone(), y_target.clone()\n\nx_context_u, y_context_u = x_context.clone(), y_context.clone()\nx_target_u, y_target_u = x_target.clone(), y_target.clone()\n\n\nlosses_r, rmses_r = [], []\nlosses_u, rmses_u = [], []\nframe_paths = []\npredictions_r, predictions_u = [], []\nfor epoch in range(num_epochs):\n    if len(x_target_r) == 0 or len(x_target_u) == 0:\n        print(\"No more target points left to acquire.\")\n        break\n\n    # Random acquisition\n    opt_random.zero_grad()\n    mu_r, sigma_r = model_random(x_context_r, y_context_r, x_target_r)\n    loss_r = NLL(mu_r, sigma_r, y_target_r)\n    loss_r.backward()\n    opt_random.step()\n\n    rmse_r = torch.sqrt(torch.mean((mu_r - y_target_r) ** 2)).item()\n    losses_r.append(loss_r.item())\n    rmses_r.append(rmse_r)\n\n    x_context_r, y_context_r, x_target_r, y_target_r = random_acquisition(\n        x_context_r, y_context_r, x_target_r, y_target_r\n    )\n\n    # Uncertainty acquisition\n    opt_uncertainty.zero_grad()\n    mu_u, sigma_u = model_uncertainty(x_context_u, y_context_u, x_target_u)\n    loss_u = NLL(mu_u, sigma_u, y_target_u)\n    loss_u.backward()\n    opt_uncertainty.step()\n\n    rmse_u = torch.sqrt(torch.mean((mu_u - y_target_u) ** 2)).item()\n    losses_u.append(loss_u.item())\n    rmses_u.append(rmse_u)\n\n    x_context_u, y_context_u, x_target_u, y_target_u = uncertainty_acquisition(\n        sigma_u, x_context_u, y_context_u, x_target_u, y_target_u\n    )\n\n    with torch.no_grad():\n      x_plot = torch.linspace(-np.pi, np.pi, 500).unsqueeze(-1).to(device)\n      mu_r, sigma_r = model_random(x_context_r, y_context_r, x_plot)\n      mu_u, sigma_u = model_uncertainty(x_context_u, y_context_u, x_plot)\n\n      predictions_r.append((mu_r.cpu().numpy(), sigma_r.cpu().numpy()))\n      predictions_u.append((mu_u.cpu().numpy(), sigma_u.cpu().numpy()))\n\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}/{num_epochs}\")\n        print(f\"  Random:     NLL={loss_r.item():.4f}, RMSE={rmse_r:.4f}\")\n        print(f\"  Uncertainty: NLL={loss_u.item():.4f}, RMSE={rmse_u:.4f}\")\n\n    if epoch % 5 == 0:\n          frame = visualize_comparison_frame(model_random, model_uncertainty,\n                                            x_context_r, y_context_r, x_target_r, y_target_r,\n                                            x_context_u, y_context_u, x_target_u, y_target_u,\n                                            epoch)\n          frame_paths.append(frame)\n\nEpoch 0/50\n  Random:     NLL=-0.2657, RMSE=0.1938\n  Uncertainty: NLL=-0.2657, RMSE=0.1938\nEpoch 10/50\n  Random:     NLL=-0.7815, RMSE=0.1068\n  Uncertainty: NLL=-0.7625, RMSE=0.1060\nEpoch 20/50\n  Random:     NLL=-0.8836, RMSE=0.0985\n  Uncertainty: NLL=-0.8110, RMSE=0.1106\nEpoch 30/50\n  Random:     NLL=-0.8785, RMSE=0.1029\n  Uncertainty: NLL=-0.9843, RMSE=0.0924\nEpoch 40/50\n  Random:     NLL=-0.8947, RMSE=0.1020\n  Uncertainty: NLL=-1.0516, RMSE=0.0880\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom vs Uncertainty Acquisition\n\n\nBoth strategies reduce the loss of the model. The uncertainty based method acquires more points from the peripheries ‚Äî that is, when \\(x\\) is close to \\(\\pi\\) or \\(-\\pi\\) whereas random method samples uniformly.\nCredits to Kaspar Martens and Deepmind for an excellent tutorial on this topic."
  }
]