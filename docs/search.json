[
  {
    "objectID": "posts/first_post/first_post.html",
    "href": "posts/first_post/first_post.html",
    "title": "Hello World üöÄ",
    "section": "",
    "text": "Hi all,\nWith the fruitful use of Youtube and AI tools, I have figured out how Quartro works so here‚Äôs my first post!\nI‚Äôll be writing here about my projects, research work, learnings and thoughts (and maybe an odd poem to bring back the WordPress feels)\nStay tuned!\nNeerja."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Learning Functions with Conditional Neural Processes\n\n\n\nNeural Process\n\nTutorial\n\nMachine Learning\n\n\n\n\n\n\n\nAug 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello World üöÄ\n\n\n\nintro\n\npersonal\n\n\n\nMy first blog post\n\n\n\nAug 25, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Conditional Neural Process/CNP.html",
    "href": "posts/Conditional Neural Process/CNP.html",
    "title": "Learning Functions with Conditional Neural Processes",
    "section": "",
    "text": "Introduction\nMeta learning models help us ‚Äòlearn how to learn‚Äô. They are models that can learn new tasks quickly from just a handful of examples. In this blog, we explore a simple but powerful framework for this: the Conditional Neural Process (CNP).\n\n\nImports\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torch.distributions import Normal\nimport torch.optim as optim\n\nnp.random.seed(42)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\nCreating training data\nInstead of training on a fixed dataset, our model learns by seeing functions. In every training step, we generate a new sine wave of the form \\(ùë¶=ùëésin(ùë•)\\) where the amplitude \\(ùëé\\) is randomly chosen from a uniform distribution in (-2,2). Then we randomly sample points from this function to be context points and target points.\n\ndef sine(x, a):\n  return a * np.sin(x)\n\ndef create_training_data(func, num_points=20, num_context=10):\n    a = np.random.uniform(-2,2) # randomly sample a\n    x_all = np.random.uniform(-np.pi, np.pi, num_points)\n    y_all = func(x_all, a)\n\n    context_indices = np.random.choice(num_points, num_context, replace=False)\n    x_context = x_all[context_indices]\n    y_context = y_all[context_indices]\n\n    # The rest are target points\n    target_indices = np.setdiff1d(np.arange(num_points), context_indices)\n    x_target = x_all[target_indices]\n    y_target = y_all[target_indices]\n\n    # Convert to PyTorch tensors\n    x_context = torch.tensor(x_context, dtype=torch.float32).unsqueeze(-1)\n    y_context = torch.tensor(y_context, dtype=torch.float32).unsqueeze(-1)\n    x_target = torch.tensor(x_target, dtype=torch.float32).unsqueeze(-1)\n    y_target = torch.tensor(y_target, dtype=torch.float32).unsqueeze(-1)\n\n    return x_context, y_context, x_target, y_target\n\nLets visualize some of these functions.\n\nx = np.linspace(-np.pi, np.pi, 200)\nplt.figure(figsize=(8, 5))\n\nfor i in range(5):\n    a = np.random.uniform(-2, 2)\n    y = sine(x, a)\n    plt.plot(x, y, label=f\"a={a:.2f}\")\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe Model\nA CNP has two main components:\n\nEncoder : A neural network that takes in context pair \\((x_i , y_i)\\) and transforms them into representation vector \\(r_i\\). To combine information from all context points, we take an average over all points and get a vector \\(r\\) of same dimension as each vector \\(r_i\\).\nDecoder: A neural network that uses the representation \\(r\\) to make predictions at target point \\(x_t\\). We pass in concatenated \\([r,x_t]\\) as the input and the output is the predicted mean and variance of \\(y_t\\). We normalize the value of sigma to make sure it is positive.\n\n\nclass Encoder(nn.Module):\n  def __init__(self, output_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(2, 128) # x_context and y_context\n    self.fc2 = nn.Linear(128, 128)\n    self.fc3 = nn.Linear(128, output_dim)\n    self.relu = nn.ReLU()\n\n  def forward(self, x):\n    x = self.relu(self.fc1(x))\n    x = self.relu(self.fc2(x))\n    x = self.fc3(x)\n    return x\n\nclass Decoder(nn.Module):\n  def __init__(self,r_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(r_dim+1, 128) # r vector and x_target concatenated\n    self.fc2 = nn.Linear(128, 2)\n    self.relu = nn.ReLU()\n\n  def forward(self, x):\n    x = self.relu(self.fc1(x))\n    x = self.fc2(x)\n    mu,sigma = x.chunk(2, dim=-1)\n    return mu, sigma\n\nclass ConditionalNeuralProcess(nn.Module):\n  def __init__(self,r_dim):\n    super().__init__()\n    self.encoder = Encoder(r_dim)\n    self.decoder = Decoder(r_dim)\n\n  def forward(self,context_x,context_y,target_x):\n\n    context_point = torch.cat([context_x, context_y], dim=-1)\n    r_i = self.encoder(context_point)\n    r =torch.mean(r_i,dim=0)\n    num_target = target_x.shape[0]\n    r_expanded = r.expand(num_target, -1)\n    decoder_input = torch.cat([r_expanded, target_x], dim=-1)\n\n    mu, sigma = self.decoder(decoder_input)\n    sigma = 0.1 + 0.9 * torch.nn.functional.softplus(sigma)\n\n    return mu, sigma\n\nr_dim = 128\nmodel = ConditionalNeuralProcess(r_dim)\n\n\nThe code below will help us visualize the model inputs and predictions.\n\ndef visualize_cnp_predictions(model, context_x, context_y, target_x, target_y):\n    model.eval()\n\n    with torch.no_grad():\n        # Generate a dense range of x-values to plot the learned function smoothly\n        x_plot = torch.linspace(target_x.min().item(), target_x.max().item(), 500).unsqueeze(-1)\n\n        mu_pred, sigma_pred = model(context_x, context_y, x_plot)\n        context_x_np = context_x.numpy()\n        context_y_np = context_y.numpy()\n        target_x_np = target_x.numpy()\n        target_y_np = target_y.numpy()\n        x_plot_np = x_plot.numpy()\n        mu_pred_np = mu_pred.numpy()\n        sigma_pred_np = sigma_pred.numpy()\n\n    plt.figure(figsize=(10, 6))\n\n    plt.scatter(context_x_np, context_y_np, c='red', label='Context Points', marker='o', s=100)\n\n    plt.scatter(target_x_np, target_y_np, c='blue', label='True Target Points', marker='x', s=100)\n    plt.plot(x_plot_np, mu_pred_np, color='green', linewidth=2, label='Predicted Mean')\n    plt.fill_between(x_plot_np.squeeze(),\n                     (mu_pred_np - 2 * sigma_pred_np).squeeze(),\n                     (mu_pred_np + 2 * sigma_pred_np).squeeze(),\n                     color='green', alpha=0.2, label='2œÉ Uncertainty')\n\n    plt.title('Conditional Neural Process Predictions', fontsize=16)\n    plt.xlabel('X', fontsize=12)\n    plt.ylabel('Y', fontsize=12)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nWhat does the model predict before training?\n\n# for i in range(5):\nx_context, y_context, x_target, y_target = create_training_data(sine)\nvisualize_cnp_predictions(model,x_context,y_context,x_target,y_target)\n\n\n\n\n\n\n\n\n\n\nTraining\nThe model is trained by minimizing the negative log-likelihood (NLL) ‚Äî encouraging the predicted distributions to assign high probability to the true target values.\n\nr_dim = 128\nmodel = ConditionalNeuralProcess(r_dim)\n\ndef NLL(mu_pred, sigma_pred, target_y):\n    # Create a normal distribution with the predicted mean and standard deviation\n    dist = Normal(mu_pred, sigma_pred)\n    log_prob = dist.log_prob(target_y)\n    loss = -torch.mean(log_prob)\n\n    return loss\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nnum_epochs = 2000\n\nfor epoch in range(num_epochs):\n    x_context, y_context, x_target, y_target = create_training_data(sine,60,10)\n    optimizer.zero_grad()\n    mu_pred, sigma_pred = model(x_context, y_context, x_target)\n    loss = NLL(mu_pred, sigma_pred, y_target)\n\n    loss.backward()\n    optimizer.step()\n\n    if (epoch) % 100 == 0:\n      visualize_cnp_predictions(model,x_context,y_context,x_target,y_target)\n      print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item()}\")\n\n\n\n\n\n\n\n\nEpoch 0/2000, Loss: 1.2191544771194458\n\n\n\n\n\n\n\n\n\nEpoch 100/2000, Loss: 1.1903642416000366\n\n\n\n\n\n\n\n\n\nEpoch 200/2000, Loss: 0.7007207274436951\n\n\n\n\n\n\n\n\n\nEpoch 300/2000, Loss: 1.5462225675582886\n\n\n\n\n\n\n\n\n\nEpoch 400/2000, Loss: 0.7815963625907898\n\n\n\n\n\n\n\n\n\nEpoch 500/2000, Loss: 0.7714107632637024\n\n\n\n\n\n\n\n\n\nEpoch 600/2000, Loss: 0.6355094313621521\n\n\n\n\n\n\n\n\n\nEpoch 700/2000, Loss: 0.5624711513519287\n\n\n\n\n\n\n\n\n\nEpoch 800/2000, Loss: 0.6095244884490967\n\n\n\n\n\n\n\n\n\nEpoch 900/2000, Loss: -0.9688698649406433\n\n\n\n\n\n\n\n\n\nEpoch 1000/2000, Loss: -0.43185102939605713\n\n\n\n\n\n\n\n\n\nEpoch 1100/2000, Loss: 0.3317843973636627\n\n\n\n\n\n\n\n\n\nEpoch 1200/2000, Loss: 1.0584346055984497\n\n\n\n\n\n\n\n\n\nEpoch 1300/2000, Loss: -0.5391824841499329\n\n\n\n\n\n\n\n\n\nEpoch 1400/2000, Loss: 0.4801075756549835\n\n\n\n\n\n\n\n\n\nEpoch 1500/2000, Loss: 0.6704821586608887\n\n\n\n\n\n\n\n\n\nEpoch 1600/2000, Loss: -0.40198639035224915\n\n\n\n\n\n\n\n\n\nEpoch 1700/2000, Loss: -0.09618251770734787\n\n\n\n\n\n\n\n\n\nEpoch 1800/2000, Loss: -0.30808529257774353\n\n\n\n\n\n\n\n\n\nEpoch 1900/2000, Loss: 0.45030006766319275\n\n\nAfter training, the model is predicting a good approximation of our function.\n\nx_context, y_context, x_target, y_target = create_training_data(sine,20,10)\nvisualize_cnp_predictions(model,x_context,y_context,x_target,y_target)\n\n\n\n\n\n\n\n\nI would say that CNPs are a rather simple approximation to neural processes. Their aggregator function being simply a mean may lose information and they do not model uncertainty as well, being a deterministic model.\nCredits to https://kasparmartens.rbind.io/post/np/ and Deepmind for an excellent tutorial on this topic."
  }
]