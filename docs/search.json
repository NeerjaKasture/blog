[
  {
    "objectID": "posts/first_post/first_post.html",
    "href": "posts/first_post/first_post.html",
    "title": "Hello World üöÄ",
    "section": "",
    "text": "Hi all,\nWith the fruitful use of Youtube and AI tools, I have figured out how Quartro works so here‚Äôs my first post!\nI‚Äôll be writing here about my projects, research work, learnings and thoughts.\nStay tuned!\nNeerja."
  },
  {
    "objectID": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html",
    "href": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html",
    "title": "Separating Multilinear Branching Programs and Formulas",
    "section": "",
    "text": "Goal: To prove a super tight polynomial separation between multilinear ABPs and multilinear arithmetic formulas. Specifically, to describe a n-variate polynomial F that is computed by a linear-size multilinear ABP but every multilinear formula computing F must be of size \\(n^{\\Omega(logn)}\\)."
  },
  {
    "objectID": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#basic-definitions",
    "href": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#basic-definitions",
    "title": "Separating Multilinear Branching Programs and Formulas",
    "section": "Basic Definitions",
    "text": "Basic Definitions\nMultilinear Polynomial: A polynomial is multilinear if it has degree at most one in each variable.\ne.g.¬†determinant, permanent, matrix product.\nMultilinear ABP: An ABP is multilinear if on every directed path from start-node to end-node no variable appears more than once.\nMultilinear formula: A formula is multilinear if every sub-formula in it computes a multilinear polynomial.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\\(f = x_1x_2 + x_2x_3\\) is multilinear but \\(f = x_1x_3+x_2^2\\) is not multilinear."
  },
  {
    "objectID": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#main-theorem",
    "href": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#main-theorem",
    "title": "Separating Multilinear Branching Programs and Formulas",
    "section": "Main Theorem",
    "text": "Main Theorem\n\n\n\n\n\n\nTheorem 1.1\n\n\n\nFor every positive integer n, there is a multilinear polynomial \\(F=F_n\\) in n variables with zero-one coefficients so that the following holds:\n\nThere is a uniform algorithm that, given n, runs in time O(n) and outputs a multilinear branching program computing F.\nOver any field, every multilinear formula computing F must be of size \\(n^{\\Omega(logn)}\\).\n\n\n\nBy setting a lower bound on the complexity of formula and an upper bound on the complexity of ABP, we can prove a polynomial separation between them.\nThere are two main ideas used in this paper:\n\nThe first is that of a full rank polynomial. A given polynomial f can be used to define a family of matrices \\(\\{M(f_\\pi)\\}_\\pi\\) , where \\(\\pi\\) ranges over all partitions of the variables X to two sets of variables Y, Z of equal size . The polynomial f is said to have full-rank if the rank of \\(M(f_\\pi)\\) is full for every such \\(\\pi\\).\nThe other is constructing a special set of partitions called arc-partitions. This is a restricted subset of all partitions n! such that it is small enough for an ABP to compute full-rank polynomials, but big enough that formulas cannot compute full rank polynomials.\n\nWe are able to make an ABP using an \\(O(n^2)\\) algorithm such that a particular partition is represented by a particular path in ABP and it computes a full rank multilinear polynomial. And we make a probabilistic argument that the polynomial computed by the formula is not full rank for most partitions with very high probability, which implies that a formula cannot compute a full rank polynomial (where the formula is of less than superpolynomial size)."
  },
  {
    "objectID": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#preliminaries",
    "href": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#preliminaries",
    "title": "Separating Multilinear Branching Programs and Formulas",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nPartial Derivative Matrix\nFor a multilinear polynomial \\(f \\in \\mathbb{F}[Y,Z]\\), the partial derivative matrix M(f) is defined such that\n\\(M_{pq}\\) = coefficient of \\(pq\\) in \\(f\\)\n\\(Y,Z\\) are two disjoint sets of variables, p is a monic multilinear monomial in Y and q is a monic multilinear monomial in Z. Size of M is \\(2^{|y|} * 2^{|z|}\\)\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\\[\nf = 3y_1z_1 + 5y_2z_2 + y_1y_2z_1z_2 \\\\\nY = \\{y_1,y_2\\}, Z=\\{z_1,z_2\\}\n\\]\nHere monic multilinear monomials in Y are \\(1,y_1,y_2,y_1y_2\\) and monic multilinear monomials in Z are \\(1,z_1,z_2,z_1z_2\\). Then the matrix M(f) will be\n\n\n\n\n1\nz1\nz2\nz1z2\n\n\n\n\n1\n0\n0\n0\n0\n\n\ny1\n0\n3\n0\n0\n\n\ny2\n0\n0\n5\n0\n\n\ny1y2\n0\n0\n0\n1\n\n\n\n\n\n\n\n\nRank of the Matrix M\nThe rank of this matrix is often used as a complexity measure for f.\nSome properties of this matrix, given polynomials \\(f,g \\in \\mathbb{F}[Y,Z]\\)\n\n\\(rank(M(f +g)) &lt;= rank(M(f))+rank(M(g))\\)\nIf f, g are over disjoint sets of variables, \\(rank(M(f.g)) = rank(M(f)). rank(M(g))\\)\n\\(rank(M(f))&lt;= 2^{min(Y(f),Z(f))}\\), where Y(f) is the number of Y variables appearing in f and Z(f) is the number of Z variables appearing in f.\n\n\n\nArc Partitions\nLet \\(X = \\{x_0,x_1...x_{n-1}\\}, Y = \\{y_0,y_1...y_{n/2}\\}\\) and \\(Z = \\{z_0,z_1...z_{n/2}\\}\\), where n is an even integer. We can define a partition \\(\\pi\\) as follows-\n\\[\n\\pi : X \\to Y \\cup Z\n\\]\ni..e a perfect matching assigning each \\(x_i\\) to a distinct variable in \\(Y \\cup Z\\). Given a polynomial f in the variables X, we can define:\n\\[\nf_\\pi = f(\\pi(x_0), \\pi(x_1) ... \\pi(x_{n-1})\n\\]\nConsider a n-cycle graph : graph with n nodes labelled \\(\\{0,1‚Ä¶n-1\\}\\) and edges between i and (i+1) modulo n.\n[i,j] ‚Üí denotes the arc between i and j i.e.¬†the set of nodes on the path {i,i+1 ‚Ä¶ j-1,j} on the path from i to j in graph, where \\(i \\neq j\\).\nThe size of the arc is the number of nodes it contains.\nLets construct a arc partition through an example. Here, a pairing is a list of disjoint pairs of nodes in the cycle and we define a distribution \\(D_p\\) on a family of pairings. A random pairing will be constructed in n/2 steps for a n-cycle graph. So, at \\(t \\in [n/2]\\), we shall have a pairing \\((P_1...P_t)\\) of the arc \\([L_t,R_t]\\)\nExample:\nTake an 8-cycle graph with \\(X = \\{0,1,2,3,4,5,6,7\\}\\). We must create 4 disjoint pairs.\nAt t=1, \\(L_1=0, R_1=1\\) by definition. So, we have arc [0,1] and \\(P_1 =\\{0,1\\}\\).\nAt t=2, we choose \\(P_2\\) by the distribution\n\\[\nP_{t+1} = \\begin{cases}\n\\{L_t- 2,L_t - 1\\} & \\text{with probability } \\frac{1}{3}, \\\\\n\\{L_t - 1, R_t+1\\} & \\text{with probability } \\frac{1}{3}, \\\\\n\\{R_t + 1, R_t + 2\\} & \\text{with probability } \\frac{1}{3}\n\\end{cases}\n\\]\nwhere all arithmetic is modulo 8. So, we have\n\\[\nP_2 = \\begin{cases}\n\\{6,7\\} (mod 8) & \\text{with probability } \\frac{1}{3}, \\\\\n\\{7, 2\\}(mod 8) & \\text{with probability } \\frac{1}{3}, \\\\\n\\{2, 3\\} & \\text{with probability } \\frac{1}{3}\n\\end{cases}\n\\]\nSay we choose \\(P_2=\\{7,2\\}(mod8)\\). The arc expands to include the pair as \\([L_{t+1},R_{t+1}] = [L_{t},R_{t}] \\cup P_{t+1}\\) . Thus, \\([L_2,R_2] = [7,2]=\\{7,0,1,2\\}\\).\n\n\n\nimage.png\n\n\nImage showing the possible choices for the next pair \\(P_{t+1}\\)\nAt t=3, we have the choices {5,6}, {6,3} and {3,4}. Suppose we choose \\(P_3=\\{3,4\\}\\), we have \\([L_3,R_3]=[7,4]=\\{7,0,1,2,3,4\\}\\).\nAt t=4, we have the choices {5,6}, {6,5} and {5,6}. Say we choose \\(P_4 = \\{5,6\\}\\), we have \\([L_4,R_4]=[7,6]=\\{7,0,1,2,3,4,5,6\\}\\).\nThe final pairing is \\(P=(P_1,P_2...P_{n/2}) = (\\{0,1\\},\\{7,2\\},\\{3,4\\},\\{5,6\\})\\).\nGiven the pairing P ~ DP, we now produce the mapping \\(\\pi\\) as follows:\nFor every pair \\(P =\\{i_t,j_t\\}, t \\in [n/2]\\) independently,\n\n\\(\\pi(x_{i_t}) = y_t, \\pi(x_{j_t}) = z_t\\) with probability 1/2\n\\(\\pi(x_{i_t}) = z_t, \\pi(x_{j_t}) = y_t\\) with probability 1/2\n\nArc partition distribution is uniform over \\(3^{n/2}.2^{n/2}\\) outcomes. This is much smaller than the n! partitions used in earlier works."
  },
  {
    "objectID": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#arc-full-rank-polynomials",
    "href": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#arc-full-rank-polynomials",
    "title": "Separating Multilinear Branching Programs and Formulas",
    "section": "Arc full rank polynomials",
    "text": "Arc full rank polynomials\nf is arc-full-rank if for every partition \\(\\pi\\), the partial derivative matrix \\(M(f(\\pi))\\) has full rank.\n\n\n\n\n\n\nTheorem 3.1\n\n\n\nIf f is an arc-full-rank multilinear polynomial in n variables over a field F, then any multilinear formula computing f over F has size at least \\(n^{o(logn)}\\)\n\n\nLet‚Äôs set up a proof for the above theorem.\nDef: A multilinear polynomial f in variables X is called a (KT)-product polynomial if there exists K disjoint sets of variables \\(X_1...X_k\\), each of size at least T, so that \\(f=f_1f_2...f_k\\) and each \\(f_k, k \\in [K]\\), is a multilinear polynomial in \\(X_k\\).\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\\(f=(x_1+2x_2)(3x_3x_4+x_3)(x_5+x_6)\\) is a (3,2)-product polynomial:\n\n\\(X_1=\\{x_1,x_2\\}, X_2=\\{x_3,x_4\\}, X_3=\\{x_5,x_6\\}\\), is a product of k=3 factors having blocks with 2 variables\n\\(f_1=(x_1+2x_2), f_2=(3x_3x_4+x_3), f_3=(x_5+x_6)\\) , each \\(f_k\\) is multilinear and uses variables from its own block\n\nNot all variables in \\(X_k\\) must appear in \\(f_k\\). eg. \\(f=x_1x_3x_5\\) is also a (3,2)-product polynomial.\n\n\n\n\n\n\n\n\n\nLemma 1 (Shpilka&Yehudayoff)\n\n\n\nAny n-variate polynomial f computed by a multilinear formula of size s can be written as a sum \\(f=f_1+f_2+...+f_{s+1}\\), each \\(f_i\\) is a (KT)-product polynomial with \\(K \\geq (logn)/100\\)and \\(T \\geq n^{7/8}\\).\n\n\nThis reduces the problem to showing each such product polynomial has low rank under a random arc-partition.\n\n\n\n\n\n\nLemma 2\n\n\n\nThere exists a constant \\(\\delta \\gt 0\\) so that the following holds. Let n be a large enough even integer. Let f be a (KT)-product polynomial in n variables with \\(K \\geq (logn)/100\\) and \\(T \\geq n^{7/8}\\). Then\n\\[\nPr[rank(M(f_\\pi)) \\geq 2^{n/2 - n^\\delta}] \\leq n^{-\\delta logn}\n\\]\nwhere \\(\\pi\\) ~ D.\n\n\ni.e., almost every arc-partition makes the rank drop exponentially.\nThe proof of this lemma will be covered soon.\nUsing these lemmas, we can prove the above theorem.\nProof by Contradiction:\nAssume there is a multilinear formula \\(\\phi\\) of size \\(s \\leq n^{(\\delta/2) logn}\\) computing a n variate arc-full-rank polynomial f.¬†Let \\(\\pi\\) be a random partition according to \\(D\\).\nThen, \\(Pr[rank(M(f_\\pi)) = 2^{n/2}] = 1\\) for an arc-full-rank f.\nFrom Lemma 1, \\(f=f_1+f_2+...+f_{s+1}\\). Let us apply the partition to this,\n\\(f_\\pi=(f_1)_\\pi+(f_2)_\\pi+...+(f_{s+1})_\\pi\\). Then, \\(M(f_{\\pi}) = M\\!\\left( \\sum_{i=1}^{s+1} (f_i)_{\\pi} \\right).\\)\n\\(Rank(M(f_{\\pi})) \\leq \\sum_{i=1}^{s+1}Rank(M\\!\\left(  (f_i)_{\\pi} \\right))\\) using property 1.\nThe sum of these s+1 non-negative terms is at least \\(2^{n/2}\\).\n\\(\\exists i \\in [s+1]\\) with rank \\(Rank(M((f_i)_\\pi)) \\geq 2^{n/2}/(s+1)\\) (Pigeonhole)\n\\[\nPr[rank(M(f_\\pi)) = 2^{n/2}] \\leq Pr[rank(M((f_i)_\\pi)) \\geq 2^{n/2}/(s+1)]\n\\]\nUsing the union bound,\n\\[\nPr[rank(M(f_\\pi)) = 2^{n/2}] \\leq \\sum_{i=1}^{s+1}Pr[rank(M((f_i)_\\pi)) \\geq 2^{n/2}/(s+1)]\n\\]\nChoose n large enough, \\(\\delta &gt; 0\\), such that \\(2^{n/2}/(s+1) &gt; 2^{n/2-n^\\delta}\\). Using lemma 2,\n\\[\n\\sum_{i=1}^{s+1}Pr[rank(M((f_i)_\\pi)) \\geq 2^{n/2-n^\\delta}] \\leq (s+1) n^{-\\delta logn}\n\\]\nSince \\(s \\leq n^{(\\delta/2) logn}\\), then \\((s+1) n^{-\\delta logn} \\leq 2n^{-\\delta/2logn}\\), as \\(n \\to \\infty\\) this becomes &lt;1. Putting it together,\n\\[\nPr[rank(M(f_\\pi)) = 2^{n/2}] &lt; 1\n\\]\nThis is a contradiction! Hence, our assumption is incorrect. Thus, we have proved part 2 of theorem 1.1."
  },
  {
    "objectID": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#construction-of-abp-for-arc-full-rank-polynomial",
    "href": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#construction-of-abp-for-arc-full-rank-polynomial",
    "title": "Separating Multilinear Branching Programs and Formulas",
    "section": "Construction of ABP for arc full rank polynomial",
    "text": "Construction of ABP for arc full rank polynomial\nSo, we will construct an ABP in which every path between start-node and end-node corresponds to a specific execution of the random process which samples arc-partitions.\nLet us construct an ABP through an example. Each node in the ABP corresponds to an arc [L,R], which sends an edge to each of the nodes [L-2,R], [L-1,R+1] and [L,R+2]. The edges have specially chosen labels that guarantee full rank w.r.t. to every arc-partition. The start node is the empty arc \\(\\phi\\), the end node is the whole cycle [n] and intermediate nodes are even sized arcs on the cycle.\nExample:\nLet us construct an ABP for 8-cycle graph. We have \\(X=\\{x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7\\}\\) and \\(\\Lambda = \\{\\lambda_e\\}\\)\nAt t=0, we know the start node is \\(\\phi\\)\nAt t=1, there is only one node [0,1]. We connect the start node to it with edge \\(\\lambda_e(x_0+x_1)\\).\nAt t&gt;1, for any node [L,R] of size 2t, we add 3 edges:\n\nEdge \\(e_1\\) between [L,R] and [L-2,R] is labelled \\(\\lambda_{e_1}(x_{L-2}+x_{L-1})\\)\nEdge \\(e_2\\) between [L,R] and [L-1,R+1] is labelled \\(\\lambda_{e_2}(x_{L-1}+x_{R+1})\\)\nEdge \\(e_3\\) between [L,R] and [L,R+2] is labelled \\(\\lambda_{e_3}(x_{R+1}+x_{R+2})\\)\n\n(Try drawing it!)\nThen, we can see that for every path \\(\\gamma\\) from start-node to end-node in the ABP, the list of edges along \\(\\gamma\\) yields a pairing P, every edge e in \\(\\gamma\\) corresponds to a pair \\(P_e=\\{i_e,j_e\\}\\) of nodes in the n-cycle. Thus,\n\\[\nF = \\sum_{\\gamma} \\prod_{e \\in \\gamma} \\lambda_e \\;\\cdot\\; \\prod_{e \\in \\gamma} (x_{i_e} + x_{j_e})\n\\]\nThis ABP is multilinear : because every path is a multiplication as above in which each \\(x_i\\) appears at most once. The size of this ABP is \\(O(n^2)\\) since there are n even sized arcs possible for each arc size and the arc sizes are 0,2,4 ‚Üí n i.e.¬†n different sizes.\nF has zero-one coefficients since given the \\(\\prod_{e \\in \\gamma} \\lambda_e\\) we can reconstruct \\(\\pi\\) (since it is unique to \\(\\gamma\\)).\nMainly, we have to show that F is arc full rank i.e.¬†for any arc-partition \\(\\pi\\), the matrix \\(M(F_\\pi)\\) has full rank \\(2^{n/2}\\). We know that the arc-partition \\(\\pi\\) comes from the pairing \\(P = P(\\pi)\\) which corresponds to a path \\(\\gamma = \\gamma(\\pi)\\). Consider the polynomial f obtained from \\(F_\\pi\\) by setting \\(\\lambda_e  = 1\\) for every e in \\(\\gamma\\), and \\(\\lambda_e  = 0\\) for every e not in \\(\\gamma\\). Then the resulting polynomial, after applying \\(\\pi\\) is:\n\\[\nf_\\pi = \\prod_{t \\in [n/2]} (y_t + z_t)\n\\]\nFor any \\(y_t,z_t\\) we can write \\(M(y_t + z_t) =¬†\\begin{pmatrix}0 & 1 \\\\1 & 0\\end{pmatrix},\\) which clearly has rank 2 over the field F. Then by property 2, we know that rank of M(f) is full. Then using Lemma 3 (below), we can say that the rank of \\(M(F_\\pi)\\) over \\(F(\\Lambda)\\) is atleast the rank of M(f) over field F. Hence, we can write the theorem:\n\n\n\n\n\n\nTheorem 3.4\n\n\n\nOver every field \\(\\mathbb{F}\\), the polynomial \\(F = F_n\\) defined above satisfies the following:\n\nF is computed by a linear-size (in the number of variables, which is \\(O(n^2)\\)) multilinear ABP. The ABP for F can be constructed uniformly in time \\(O(n^2)\\).\nF has zero-one coefficients.\nF is arc-full-rank as a polynomial in the variables X over the field \\(\\mathbb{F}(\\Lambda)\\) of rational functions in \\(\\Lambda\\).\n\n\n\n\n\n\n\n\n\nLemma 3\n\n\n\nLet K be a field and consider L, the field of rational functions \\(K(\\lambda_1,\\ldots,\\lambda_m)\\). If f is a polynomial in L[Y,Z] and \\(a_1,\\ldots,a_m\\) are elements of K, let g be the polynomial in K[Y,Z] obtained from f by substituting \\(\\lambda_i\\) with \\(a_i\\). Then the rank over K of M(g) is at most the rank over L of M(f).\n\n\nThus, we have been able to describe a n-variate polynomial f which can be computed by the linear-size ABP F but cannot be computed by a multilinear formula \\(\\phi\\) of size \\(s \\leq n^{\\Omega(logn)}\\). Thus, we have proved part 1 of theorem 1.1."
  },
  {
    "objectID": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#arc-partitions-and-product-polynomials",
    "href": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#arc-partitions-and-product-polynomials",
    "title": "Separating Multilinear Branching Programs and Formulas",
    "section": "Arc-partitions and product polynomials",
    "text": "Arc-partitions and product polynomials\nWe still need to prove Lemma 2. Let‚Äôs restate it:\nLemma 2: There exists a constant \\(\\delta \\gt 0\\) so that the following holds. Let n be a large enough even integer. Let f be a (KT)-product polynomial in n variables with \\(K \\geq (logn)/100\\) and \\(T \\geq n^{7/8}\\). Then \\(P[rank(M(f_\\pi)) \\geq 2^{n/2 - n^\\delta}] \\leq n^{-\\delta logn}\\) where \\(\\pi\\) ~ D.\nLet S be a partition of the n-cycle to K parts, namely \\(S = (S_1, S_2 \\dots S_k)\\) where \\(\\bigcup_{k \\in [K]} S_k\\) is the whole cycle and \\(S_k \\cap S_{k'} = \\varnothing \\quad \\text{for all } k \\ne k' \\text{ in } [K].\\)\nThink of [K] as a set of colors and S as a coloring of the cycle.\nFor a pairing P, define the number of k-violations as:\n$$ V_k(P) = {, P_t P : P_t S_k = 1 ,}\n$$\ni.e.¬†the set of pairs in which one color is k and the other color is different.\n\\[\nG(P) = \\bigl|\\{\\, k \\in [K] : |V_k(P)| \\ge n^{1/1000} \\,\\}\\bigr|.\n\\]\nV(P) defines how many edges leave color k and G(P) is the set of colors that have lots of such edges.\n\n\n\n\n\n\nExample:\n\n\n\n\n\nn =8, k =3. \\(P= (\\{0,1\\},\\{7,2\\},\\{3,4\\},\\{5,6\\})\\)\n\\(S_1=\\{0,1,4\\}\\) (red), \\(S_2=\\{2,5\\}\\) (blue), \\(S_3=\\{3,6,7\\}\\) (green).\n\n\n\n8cycle.png\n\n\nFor k=1, lets go through the pairs in P:\n\n{0,1} both are in \\(S_1\\)\n{7,2} both are not in \\(S_1\\)\n{3,4} only 4 in \\(S_1\\)\n{5,6} both not in \\(S_1\\)\n\nHence \\(|V_1(P)| = 1\\).\nSimilarly, \\(|V_2(P)|=2,|V_3(P)|=2.\\)\nHere n=8, so \\(n^{1/1000}\\) is roughly 1. Then, \\(G(P)=3\\).\nWe can think of a color as a group, and a node in the color group with an edge to another color‚Äôs node has a cross edge.\n\n\n\n\n\n\n\n\n\nLemma 4\n\n\n\nThere exists a constant C &gt; 0 such that for all \\(C \\leq K \\leq n^{1/1000}\\) the following holds: Let \\(S = (S_1, S_2 \\dots S_k)\\) be a partition of the n-cycle and suppose that \\(|S_k| \\geq n^{7/8} \\forall k \\in [K]\\). Then,\n\\[\nPr[G(P)\\leq K/1000]\\leq n^{-\\Omega(K)}\n\\]\nwhere P ~ \\(D_P\\).\n\n\nWe will prove this lemma in the next section. It is saying that the probability that few colors have many cross edges is very low. Or equivalently, with overwhelming probability, many colors have many cross-edges.\nProof:\nLet f be a (K,T)-product with \\(K = \\left\\lceil \\frac{\\log n}{100} \\right\\rceil\n, T = \\left\\lceil n^{7/8} \\right\\rceil\\) , partition S of the cycle made by variables of f.¬†Let P~DP, \\(\\pi\\)~D be a random partition obtained from P. For every P, define a graph H(P) such that:\n\nNodes of H(P) are colors k in [K] so that \\(|V_k(P)| \\geq n^{1/1000}\\)\nEvery two nodes \\(k \\neq k'\\) in G(P) are connected by an edge if \\(|V_k(P) \\cap V_{k'}(P)| \\geq n^{1/1500}\\)\nThe degree of each node in H(P) is atleast 1. If degree of node k in H(P) is 0, no more than \\(n^{1/1500}\\) edges connect it to k‚Äô. Then the total number of pairs from k to all other colors in H(P) is \\(\\leq (G(P)-1).n^{1/1500}\\) that is \\(\\leq k.n^{1/1500}\\). Then the total number of edges from k color node is \\(|V_k(P)| \\leq k.n^{1/1500} = logn.n^{1/1500}\\). But \\(V_k(P)\\geq n^{1/1000}\\).\n\nClaim: Let H be a graph with minimal degree at least one and M nodes, then there is a subset \\(\\{h_1 \\dots h_N\\}\\) of the nodes of H of size \\(N \\geq M/2 -1\\), so that for every \\(j \\in [N-1]\\), the degree of \\(h_{j+1}\\) in the graph induced on the nodes not in \\(\\{h_1 \\dots h_j\\}\\) is at least one.\nProof by induction: Base case M‚â§2, if M = 1, degree ‚â•1 not possible! If M=2, the two nodes are connected to each other, we can pick either one and we are done.\nSo for M &gt; 2, Pick \\(h_1\\) - the node having least degree, and consider the graph \\(H_1\\) having removed that node.\n\\(H_1\\) has at most one isolated (degree=0) node, lets call it \\(h_1'\\).\nNow consider the graph removing \\(\\{h_1,h_1'\\}\\) having M-2 nodes which would also have minimal degree ‚â• 1. We can say that there is a subset \\(\\{h_2 \\dots h_N\\}\\) to satisfy the claim where N ‚â• M/2 -2. We can add \\(h_1\\) back to get N+1 ‚â• M/2 -1. At each step we can remove 1 or 2 nodes such that we get a sequence \\(\\{h_1 \\dots h_N\\}\\) that satisfies the claim.\nNow we have H(P) with M=|G(P)| and minimal degree‚â•1. Then there is a subset of colors \\(\\{k_1 \\dots k_{K'}\\}\\) with K‚Äô ‚â• |G(P)|/2 -1 such that the claim above is fulfilled.\nView the sampling of \\(\\pi\\) from P as happening in the sequence \\(k_1 \\dots k_{K'}\\), first for all pairs touching \\(k_1\\), then for remaining pairs touching \\(k_2\\) and so on. For each color \\(k_j\\) that has not been considered yet, there are still many cross pairs connecting it to other colors.\nFor every \\(j \\in [K]\\), define \\(E_j\\) to be the event that \\(|Y_{k_j}-|S_{k_j}|/2| \\leq n^{1/5000}\\), where \\(Y_{k_j}=|\\pi^{-1}(Y)\\cap S_{k_j}|\\). Remember, this \\(Y_{k_j}\\) just means variables in X that will map to variables in Y and that have color \\(k_j\\). So, the event \\(E_j\\) means that the color \\(k_j\\) is roughly balanced between Y and Z. Since \\(|S_k| \\geq n^{7/8}\\) this is a pretty small error window around half.\nNow, conditioned on \\(E_1 \\dots E_j\\), there are at least \\(n^{1/1500}\\) pairs \\(P_t\\) such that \\(|P_t \\cap S_{k_j}|=1\\) are not yet assigned variables in Y,Z (since at step j+1, color \\(k_{j+1}\\) still has a neighbour k‚Äô in \\(k_{j+2}...k_{K'}\\).\nThen, the element in \\(P_t \\cap S_{k_j}\\) is assigned a Y variable with probability 1/2, for each, independently of any other \\(P_t\\). Then for this event, the universe is \\(U \\geq n^{1/1500}\\) and the marginal is 1/2, which is like a binomial random variable B with U independent fair coin tosses. Then for any specific k,\n\\[\nPr[B=k]=O(1/\\sqrt(U)) \\leq O(n^{-1/3000})\n\\]\nHence, for all \\(j \\in [K]\\), we find the conditionally probability for event \\(E_j\\),\n\\[\n\\begin{align}\n\\mathbb{P}[E_j \\mid E_1, \\ldots, E_{j-1}, P] &\\le \\mathbb{P}_B\\!\\left[\\, U/2 - n^{1/5000} \\;\\le\\; B \\;\\le\\; U/2 + n^{1/5000} \\,\\right] \\\\\n&\\le O\\!\\left( 2 n^{1/5000} n^{-1/3000} \\right) \\\\\n&\\le n^{-\\Omega(1)}\n\\end{align}\n\\]\nBy chain rule, \\(\\mathbb{P}[\\text{all balanced} \\mid P] \\le n^{-\\Omega(G(P))}\\). Average this over all pairings for G(P) ‚â• K/1000,\n\\(\\mathbb{P}[\\text{all balanced}‚à£G(P)&gt;K/1000]‚â§\\mathbb{E}\\!\\left[\\, n^{-\\Omega(G(P))} \\,\\middle|\\, G(P) &gt; K/1000 \\right] \\leq n^{-\\Omega(k)}\\)\nWe can use Bayes theorem,\n\\[\n\\begin{align}\n\\mathbb{P}[\\text{all colors balanced}] &= \\mathbb{P}[\\text{all balanced} \\mid G(P) &gt; K/1000] \\cdot \\mathbb{P}[G(P) &gt; K/1000] \\\\\n&\\quad + \\mathbb{P}[\\text{all balanced} \\mid G(P) \\le K/1000] \\cdot \\mathbb{P}[G(P) \\le K/1000]\n\\end{align}\n\\]\nUsing Lemma 4,\n\\[\n\\begin{align}\n\\mathbb{P}\\!\\left[\\, |Y_k - |S_k|/2| \\le n^{1/5000} \\;\\text{ for all } k \\in [K] \\,\\right] &\\le \\mathbb{E}\\!\\left[\\, n^{-\\Omega(G(P))} \\,\\middle|\\, G(P) &gt; K/1000 \\right] + n^{-\\Omega(K)} \\\\\n&= n^{-\\Omega(\\log n)}\n\\end{align}\n\\]\nUsing Property 2&3, \\(\\operatorname{rank}(M(f^{\\Pi}))\\;\\le\\;\\prod_{k=1}^{K} 2^{\\min(Y_k, Z_k)}\\;=\\;2^{\\sum_{k=1}^{K} \\min(Y_k, Z_k)}\\)\n\\(\\min(Y_k, Z_k)\n= \\frac{Y_k + Z_k}{2} - \\frac{\\lvert Y_k - Z_k\\rvert}{2},\nY_k + Z_k = |S_k|\\), then \\(\\min(Y_k, Z_k)\n= \\frac{|S_k|}{2} - \\lvert Y_k-\\frac{|S_k|}{2}\\rvert\\)\nThen, \\(\\sum_{k=1}^{K} \\min(Y_k, Z_k) = \\frac{n}{2} - \\sum_{k=1}^{K} \\lvert Y_k-\\frac{|S_k|}{2}\\rvert\\).\nSuppose there is some k that is unbalanced, \\(\\sum_{k=1}^{K} \\lvert Y_k-\\frac{|S_k|}{2}\\rvert \\geq \\lvert Y_k-\\frac{|S_k|}{2}\\rvert \\gt n^{1/5000}\\).\nThen, \\(\\operatorname{rank}(M(f^{\\Pi})) \\lt 2^{\\,n/2 - n^{1/5000}}\\), or taking the contrapositive, we get the below equation\n\\[\n\\mathbb{P}\\!\\left[\\, \\operatorname{rank}(M(f_{\\Pi})) \\ge 2^{\\,n/2 - n^{1/5000}} \\right]\\;\\le\\;\\mathbb{P}\\!\\left[\\, |Y_k - |S_k|/2| \\le n^{1/5000} \\;\\text{ for all } k \\in [K] \\,\\right] \\leq n^{-\\Omega(\\log n)}\n\\]\nHence, we have proved Lemma 3."
  },
  {
    "objectID": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#conclusion",
    "href": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#conclusion",
    "title": "Separating Multilinear Branching Programs and Formulas",
    "section": "Conclusion",
    "text": "Conclusion\nThe proof for Lemma 4 is remaining, but I leave it for the future.\nThe main insight is that multilinear formulas necessarily break their input variables into many ‚Äúcolor blocks‚Äù, and for the polynomial to have full rank under any partition, each block must split almost perfectly evenly between Y and Z. But a random arc-partition almost never splits all blocks evenly; since the probability that the binomial will land in a tiny window around its mean is small to evenly split any given block is small, and for this to happen for a k blocks is very unlikely. This forces every small formula to fail rank-fullness, while the explicitly constructed ABP-based polynomial always maintains full rank. Hence formulas need superpolynomial size, whereas ABPs do not."
  },
  {
    "objectID": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#citation",
    "href": "posts/ACT/Separating Multilinear Branching Programs and Form 7c05f66a1beb46849f8ac054607fcbcb.html#citation",
    "title": "Separating Multilinear Branching Programs and Formulas",
    "section": "Citation",
    "text": "Citation\nZeev Dvir, Guillaume Malod, Sylvain Perifel, and Amir Yehudayoff. 2012. Separating multilinear branching programs and formulas. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing (STOC ‚Äô12). Association for Computing Machinery, New York, NY, USA, 615‚Äì624. https://doi.org/10.1145/2213977.2214034\nA. Shpilka and A. Yehudayo . Arithmetic circuits: A survey of recent results and open questions. Found. Trends Theor. Comput. Sci. 5, pages 207388, 2010.\nP.S. I have maintained the same theorem numbering from the paper, but the lemma numbering is mismatched."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Separating Multilinear Branching Programs and Formulas\n\n\n\nAlgebraic Complexity Theory\n\n\n\nPaper Presentation: proving a super tight poly-separation between multilinear ABPs and multilinear formulas\n\n\n\nNov 25, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning Functions with Conditional Neural Processes\n\n\n\nMachine Learning\n\nNeural Processes\n\ntutorial\n\n\n\nImplementing a Conditional Neural Process model to learn the sine wave function\n\n\n\nAug 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello World üöÄ\n\n\n\nintro\n\npersonal\n\n\n\nMy first blog post\n\n\n\nAug 25, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Conditional Neural Process/CNP.html",
    "href": "posts/Conditional Neural Process/CNP.html",
    "title": "Learning Functions with Conditional Neural Processes",
    "section": "",
    "text": "Introduction\nMeta learning models help us ‚Äòlearn how to learn‚Äô. They are models that can learn new tasks quickly from just a handful of examples. In this blog, we explore a simple but powerful framework for this: the Conditional Neural Process.\n\n\nImports and Setup\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import Normal\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\nCreating training data\nInstead of training on a fixed dataset, our model learns by seeing functions. In every training step, we generate a new sine wave of the form \\(ùë¶=ùëésin(ùë•)\\) where the amplitude \\(ùëé\\) is randomly chosen from a uniform distribution in (-2,2). Then we randomly sample points from this function to be context points and target points. We have also added random gaussian noise to our data.\n\ndef sine(x, a):\n  return a * np.sin(x)\n\ndef create_training_data(func, a=None, num_points=None, num_context=None):\n    if not a:\n      a = np.random.uniform(-2,2) # randomly sample a\n\n    if not num_points:  # if not specified choose a random number of context and target points\n        num_points = np.random.randint(50, 100)\n    if not num_context:\n        num_context = np.random.randint(10, 20)\n\n    x_all = np.random.uniform(-np.pi, np.pi, num_points)\n    y_all = func(x_all, a)\n\n    noise = np.random.normal(0, 0.1, size=y_all.shape) # add noise\n    y_all += noise\n\n    context_indices = np.random.choice(num_points, num_context, replace=False)\n    x_context = x_all[context_indices]\n    y_context = y_all[context_indices]\n\n    target_indices = np.setdiff1d(np.arange(num_points), context_indices)\n    x_target = x_all[target_indices]\n    y_target = y_all[target_indices]\n\n    x_context = torch.tensor(x_context, dtype=torch.float32).unsqueeze(-1).to(device)\n    y_context = torch.tensor(y_context, dtype=torch.float32).unsqueeze(-1).to(device)\n    x_target = torch.tensor(x_target, dtype=torch.float32).unsqueeze(-1).to(device)\n    y_target = torch.tensor(y_target, dtype=torch.float32).unsqueeze(-1).to(device)\n\n    return x_context, y_context, x_target, y_target\n\nLets visualize some of these functions:\n\n\n\n\n\n\n\n\n\n\n\nThe Model\nA CNP has two main components:\n\nEncoder : A neural network that takes in context pair \\((x_i , y_i)\\) and transforms them into representation vector \\(r_i\\). To combine information from all context points, we take an average over all points and get a vector \\(r\\) of same dimension as each vector \\(r_i\\).\nDecoder: A neural network that uses the representation \\(r\\) to make predictions at target point \\(x_t\\). We pass in concatenated \\([r,x_t]\\) as the input and the output is the predicted mean and variance of \\(y_t\\). We normalize the value of sigma to make sure it is positive.\n\n\nclass Encoder(nn.Module):\n  def __init__(self, output_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(2, 128) # (x_context, y_context) as input\n    self.fc2 = nn.Linear(128, 128)\n    self.fc3 = nn.Linear(128, output_dim)\n    self.relu = nn.ReLU()\n\n  def forward(self, x):\n    x = self.relu(self.fc1(x))\n    x = self.relu(self.fc2(x))\n    x = self.fc3(x)\n    return x\n\nclass Decoder(nn.Module):\n  def __init__(self,r_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(r_dim+1, 128) # (r vector, x_target) concatenated as input\n    self.fc2 = nn.Linear(128, 2)\n    self.relu = nn.ReLU()\n\n  def forward(self, x):\n    x = self.relu(self.fc1(x))\n    x = self.fc2(x)\n    mu,log_sigma = x.chunk(2, dim=-1)\n    return mu, log_sigma\n\nclass ConditionalNeuralProcess(nn.Module):\n  def __init__(self,r_dim):\n    super().__init__()\n    self.encoder = Encoder(r_dim)\n    self.decoder = Decoder(r_dim)\n\n  def forward(self,context_x,context_y,target_x):\n\n    context_point = torch.cat([context_x, context_y], dim=-1)\n    r_i = self.encoder(context_point)\n    r =torch.mean(r_i,dim=0)\n    num_target = target_x.shape[0]\n    r_expanded = r.expand(num_target, -1)\n    decoder_input = torch.cat([r_expanded, target_x], dim=-1)\n\n    mu, log_sigma = self.decoder(decoder_input)\n    sigma = torch.exp(log_sigma) # variance must be positive\n\n    return mu, sigma\n\nr_dim = 128\nmodel = ConditionalNeuralProcess(r_dim).to(device)\n\nThe code below will help us visualize the model inputs and predictions.\n\ndef visualize_cnp_predictions(model, context_x, context_y, target_x, target_y):\n    model.eval()\n\n    with torch.no_grad():\n        # Generate a dense range of x-values to plot the learned function smoothly\n        x_plot = torch.linspace(-np.pi, np.pi, 500, device=device).unsqueeze(-1)\n\n\n        mu_pred, sigma_pred = model(context_x, context_y, x_plot)\n\n        context_x_np = context_x.cpu().numpy()\n        context_y_np = context_y.cpu().numpy()\n        target_x_np = target_x.cpu().numpy()\n        target_y_np = target_y.cpu().numpy()\n        x_plot_np = x_plot.cpu().numpy()\n        mu_pred_np = mu_pred.cpu().numpy()\n        sigma_pred_np = sigma_pred.cpu().numpy()\n\n    plt.figure(figsize=(10, 6))\n\n    plt.scatter(context_x_np, context_y_np, c='red', label='Context Points', marker='o', s=70)\n    plt.scatter(target_x_np, target_y_np, c='blue', label='True Target Points', marker='x', s=70)\n\n    plt.plot(x_plot_np, mu_pred_np, color='green', linewidth=2, label='Predicted Mean')\n    plt.fill_between(\n        x_plot_np.squeeze(),\n        (mu_pred_np - 2 * sigma_pred_np).squeeze(),\n        (mu_pred_np + 2 * sigma_pred_np).squeeze(),\n        color='green', alpha=0.2, label='Variance'\n    )\n\n    plt.title('Conditional Neural Process Predictions')\n    plt.xlabel('X', fontsize=12)\n    plt.ylabel('Y', fontsize=12)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nLets sample data for an example function with a = 1.5 , What does our model‚Äôs prediction look like before training?\n\n\n\n\n\n\n\n\n\n\n\nTraining\nThe model is trained by minimizing the negative log-likelihood (NLL) ‚Äî encouraging the predicted distributions to assign high probability to the true target values.\n\nr_dim = 128\nmodel = ConditionalNeuralProcess(r_dim).to(device)\n\ndef NLL(mu_pred, sigma_pred, target_y):\n    dist = Normal(mu_pred, sigma_pred)\n    log_prob = dist.log_prob(target_y)\n    loss = -torch.mean(log_prob)\n\n    return loss\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nnum_epochs = 10000\nlosses=[]\npredictions_example=[]\nx_plot_example = torch.linspace(-np.pi, np.pi, 500, device=device).unsqueeze(-1)\n\nfor epoch in range(num_epochs):\n    x_context, y_context, x_target, y_target = create_training_data(sine)\n    optimizer.zero_grad()\n    mu_pred, sigma_pred = model(x_context, y_context, x_target)\n    loss = NLL(mu_pred, sigma_pred, y_target)\n    losses.append(loss.item())\n\n    loss.backward()\n    optimizer.step()\n\n    if (epoch+1) % 1000 == 0:\n      #visualize_cnp_predictions(model,x_context_example,y_context_example,x_target_example,y_target_example)\n      print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n\n    with torch.no_grad():\n        mu, sigma = model(x_context_example, y_context_example, x_plot_example)\n        predictions_example.append((mu.cpu().numpy(), sigma.cpu().numpy()))\n\nEpoch 1000/10000, Loss: 0.47015589475631714\nEpoch 2000/10000, Loss: -0.42924317717552185\nEpoch 3000/10000, Loss: -0.12417580932378769\nEpoch 4000/10000, Loss: -0.6538487076759338\nEpoch 5000/10000, Loss: -0.4501391053199768\nEpoch 6000/10000, Loss: -0.36041417717933655\nEpoch 7000/10000, Loss: -0.09112993627786636\nEpoch 8000/10000, Loss: -0.6509308218955994\nEpoch 9000/10000, Loss: -0.7228361368179321\nEpoch 10000/10000, Loss: -0.17287860810756683\n\n\nThe losses for the overall training process oscillate a lot but we can see a downward trend, as in the plot below.\n\n\n\n\n\n\n\n\n\nWe can visualize how the training makes our model converge to the example function using a short animation. Observe how the shaded green area indicating variance becomes smaller as our model becomes more accurate and confident.\n\n\n\n\n\n\n\nConclusion\nSo, after training, the model is predicting a good approximation of our function.\n\n\n\n\n\n\n\n\n\nI would say that CNPs are a rather simple approximation to neural processes. Their aggregator function being simply a mean may lose information and they do not model uncertainty as well, being a deterministic model. However they are simple to train and perform reasonably well on simple functions such as the sine wave.\n\n\nActive Learning Loop\nThe CNP trained above can help us in choosing the next points to acquire in order to make even better predictions about a particular sine wave. Below is the code for two acquisitions strategies - a random strategy and an uncertainty based strategy which chooses point with maximum variance.\n\ndef random_acquisition(x_context, y_context, x_target, y_target):\n    idx = np.random.randint(len(x_target))\n\n    x = x_target[idx:idx+1]\n    y = y_target[idx:idx+1]\n\n    x_context_new = torch.cat([x_context, x], dim=0)\n    y_context_new = torch.cat([y_context, y], dim=0)\n\n    x_target_new = torch.cat([x_target[:idx], x_target[idx+1:]], dim=0)\n    y_target_new = torch.cat([y_target[:idx], y_target[idx+1:]], dim=0)\n\n    return  x_context_new, y_context_new, x_target_new, y_target_new\n\ndef uncertainty_acquisition(sigma, x_context, y_context, x_target, y_target):\n    idx = torch.argmax(sigma).item()\n\n    x = x_target[idx:idx+1]\n    y = y_target[idx:idx+1]\n\n    x_context_new = torch.cat([x_context, x], dim=0)\n    y_context_new = torch.cat([y_context, y], dim=0)\n\n    x_target_new = torch.cat([x_target[:idx], x_target[idx+1:]], dim=0)\n    y_target_new = torch.cat([y_target[:idx], y_target[idx+1:]], dim=0)\n\n    return  x_context_new, y_context_new, x_target_new, y_target_new\n\nLets try this on the example function above with a=1.5 We start with 2 context points and make one target point become context point per epoch.\n\nnum_epochs = 50\na=1.5\nx_context, y_context, x_target, y_target = create_training_data(sine, a,num_points=100, num_context=2)\n\ndef rmse_loss(y_pred,y_target):\n  return torch.sqrt(torch.mean((y_pred - y_target) ** 2)).item()\n\n\nimport copy\n\nmodel_random = copy.deepcopy(model)\nmodel_uncertainty = copy.deepcopy(model)\n\nopt_random = torch.optim.Adam(model_random.parameters(), lr=1e-3)\nopt_uncertainty = torch.optim.Adam(model_uncertainty.parameters(), lr=1e-3)\n\nx_context_r, y_context_r = x_context.clone(), y_context.clone()\nx_target_r, y_target_r = x_target.clone(), y_target.clone()\n\nx_context_u, y_context_u = x_context.clone(), y_context.clone()\nx_target_u, y_target_u = x_target.clone(), y_target.clone()\n\n\nlosses_r, rmses_r = [], []\nlosses_u, rmses_u = [], []\nframe_paths = []\npredictions_r, predictions_u = [], []\nfor epoch in range(num_epochs):\n    if len(x_target_r) == 0 or len(x_target_u) == 0:\n        print(\"No more target points left to acquire.\")\n        break\n\n    # Random acquisition\n    opt_random.zero_grad()\n    mu_r, sigma_r = model_random(x_context_r, y_context_r, x_target_r)\n    loss_r = NLL(mu_r, sigma_r, y_target_r)\n    loss_r.backward()\n    opt_random.step()\n\n    rmse_r = torch.sqrt(torch.mean((mu_r - y_target_r) ** 2)).item()\n    losses_r.append(loss_r.item())\n    rmses_r.append(rmse_r)\n\n    x_context_r, y_context_r, x_target_r, y_target_r = random_acquisition(\n        x_context_r, y_context_r, x_target_r, y_target_r\n    )\n\n    # Uncertainty acquisition\n    opt_uncertainty.zero_grad()\n    mu_u, sigma_u = model_uncertainty(x_context_u, y_context_u, x_target_u)\n    loss_u = NLL(mu_u, sigma_u, y_target_u)\n    loss_u.backward()\n    opt_uncertainty.step()\n\n    rmse_u = torch.sqrt(torch.mean((mu_u - y_target_u) ** 2)).item()\n    losses_u.append(loss_u.item())\n    rmses_u.append(rmse_u)\n\n    x_context_u, y_context_u, x_target_u, y_target_u = uncertainty_acquisition(\n        sigma_u, x_context_u, y_context_u, x_target_u, y_target_u\n    )\n\n    with torch.no_grad():\n      x_plot = torch.linspace(-np.pi, np.pi, 500).unsqueeze(-1).to(device)\n      mu_r, sigma_r = model_random(x_context_r, y_context_r, x_plot)\n      mu_u, sigma_u = model_uncertainty(x_context_u, y_context_u, x_plot)\n\n      predictions_r.append((mu_r.cpu().numpy(), sigma_r.cpu().numpy()))\n      predictions_u.append((mu_u.cpu().numpy(), sigma_u.cpu().numpy()))\n\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}/{num_epochs}\")\n        print(f\"  Random:     NLL={loss_r.item():.4f}, RMSE={rmse_r:.4f}\")\n        print(f\"  Uncertainty: NLL={loss_u.item():.4f}, RMSE={rmse_u:.4f}\")\n\n    if epoch % 5 == 0:\n          frame = visualize_comparison_frame(model_random, model_uncertainty,\n                                            x_context_r, y_context_r, x_target_r, y_target_r,\n                                            x_context_u, y_context_u, x_target_u, y_target_u,\n                                            epoch)\n          frame_paths.append(frame)\n\nEpoch 0/50\n  Random:     NLL=-0.2657, RMSE=0.1938\n  Uncertainty: NLL=-0.2657, RMSE=0.1938\nEpoch 10/50\n  Random:     NLL=-0.7815, RMSE=0.1068\n  Uncertainty: NLL=-0.7625, RMSE=0.1060\nEpoch 20/50\n  Random:     NLL=-0.8836, RMSE=0.0985\n  Uncertainty: NLL=-0.8110, RMSE=0.1106\nEpoch 30/50\n  Random:     NLL=-0.8785, RMSE=0.1029\n  Uncertainty: NLL=-0.9843, RMSE=0.0924\nEpoch 40/50\n  Random:     NLL=-0.8947, RMSE=0.1020\n  Uncertainty: NLL=-1.0516, RMSE=0.0880\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom vs Uncertainty Acquisition\n\n\nBoth strategies reduce the loss of the model. The uncertainty based method acquires more points from the peripheries ‚Äî that is, when \\(x\\) is close to \\(\\pi\\) or \\(-\\pi\\) whereas random method samples uniformly.\nCredits to Kaspar Martens and Deepmind for an excellent tutorial on this topic."
  }
]